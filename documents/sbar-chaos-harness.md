# SBAR Chaos Harness Runbook

This note captures how the SBAR chaos harness now behaves and the expectations for keeping it working. Treat it as the ‚Äúdo not break‚Äù file for the progressive SBAR workflow.

## What the harness does

- Replays JSONL dialogue (`_validation/test_dialogue.jsonl` or any scene) through `generate_sbar_report` and the progressive logger.
- Calls the Medicine LLM via LM Studio (`LLM_API_URL`, default `http://127.0.0.1:1234/v1/chat/completions`).
- Produces *two* Markdown artifacts per run:
  - `summary.md` ‚Äì iteration-level SBAR handoff summaries.
  - `progress.md` ‚Äì timeline view with every SBAR snapshot and the supervisor critique under each turn.
- Emits metrics to `_validation/orchestrator_metrics.jsonl` with `event="sbar_chaos"`.

## Output layout

```
_validation/
  sbar_chaos_logs/
    <scene>/
      <YYYYMMDD-HHMMSSZ>/
        summary.md
        progress.md
```

- Folder names are UTC timestamps, so there are no opaque UUIDs to hunt down.
- `progress.md` is append-only for the run and uses snapshot labels like `## SBAR Snapshot 1.3` (iteration 1, turn 3).
- The CLI summary (`_validation/sbar_chaos_summary.md`) points to these locations after every run.

## Retention and archiving

- By default the harness keeps the **five** most recent runs per scene. Older runs are moved to `_validation/sbar_chaos_logs/archive/<scene>/` (safe to delete).
- Override at runtime:
  - CLI flag: `python tools/run_sbar_chaos.py --retain 2 ...`
  - Environment variable: `Set-Item Env:SBAR_CHAOS_RETAIN 10`
- If you need to preserve a run permanently, copy the timestamped folder somewhere else before the archive task sweeps it up.

## Running locally

```powershell
Set-Item Env:PYTHONPATH '.;src'
Set-Item Env:PYTHONIOENCODING 'utf-8'
Set-Item Env:LLM_API_URL 'http://127.0.0.1:1234/v1/chat/completions'
Set-Item Env:WITH_LLM '1'
python tools/run_sbar_chaos.py --scene scenes/tension_pneumo/dialogue.jsonl --with-llm --iters 1
```

If LM Studio is offline the harness downgrades to stub output and clearly labels the Markdown so you notice.

## Guardrails (‚Äúdo not break‚Äù checklist)

1. **Keep the directory contract** ‚Äì downstream tooling expects the timestamped folder with `summary.md` and `progress.md`. Do not rename without updating the CLI and tests.
2. **Preserve telemetry fields** ‚Äì dashboards look for `progress_path`, `run_dir`, and `run_started` in metrics; always include these keys when enhancing the harness.
3. **Write tests before refactors** ‚Äì `tests/test_sbar_chaos_harness.py` asserts the structure; extend that test rather than hand-editing Markdown templates.
4. **Archive, don‚Äôt delete** ‚Äì use the retention knob instead of manually clearing `_validation/sbar_chaos_logs/`. That ensures investigators can recover runs if needed.
5. **Document prompt changes** ‚Äì if you tweak LLM prompts or response parsing, record the rationale here so the next engineer understands the contract.

Following the list above keeps the SBAR heartbeat healthy and reproducible. If you need to deviate, update this file so the next runbooks stay accurate.

## Viewing Results

To inspect recent SBAR simulations:

1. Run: `python -m src.sbar_dashboard.app`
2. Open: <http://127.0.0.1:8010/dashboard>
3. Click on a scene/run to view its Markdown logs.

The dashboard reads the local `_validation/sbar_chaos_logs/` and `_validation/orchestrator_metrics.jsonl` files only; no external network connectivity or additional services are required.

Each SBAR simulation now concludes with a **Final Scene Summary** section generated by the LLM. The model returns JSON shaped like:

```json
{
  "summary": "Concise narrative of the case progression",
  "diagnostic_impression": ["Diagnosis A", "Diagnosis B", "Diagnosis C"],
  "lessons": ["Lesson one", "Lesson two"],
  "final_recommendation": "Clear next steps for the team"
}
```

The Markdown renders this as:

```
## ü©∫ Final Scene Summary
**Overall Summary:** ...
**Diagnostic Impression:** ...
**Lessons:** ...
**Final Recommendation:** ...
```

This block appears at the end of both `progress.md` and `summary.md`, providing a run-level meta-critique you can review or archive.

## Dataset Export

To generate structured datasets for analytics or training:

1. Ensure recent runs exist under `_validation/sbar_chaos_logs/`.
2. Call the exporter:

   ```python
   from pathlib import Path
   from src.utils.sbar_exporter import export_sbar_dataset

   metadata = export_sbar_dataset(
       Path("_validation/sbar_chaos_logs"),
       Path("_validation/datasets"),
       ["yaml", "jsonl"],
   )
   print(metadata)
   ```

3. The exporter merges `progress.md`, `summary.md`, and `orchestrator_metrics.jsonl` into `sbar_dataset.yaml` and `sbar_dataset.jsonl`.

Each record contains:

```json
{
  "scene": "tension_pneumo",
  "timestamp": "20251018-024033Z",
  "snapshot_id": "1.3",
  "sbar": {"situation": "...", "background": "...", "assessment": "...", "recommendation": "..."},
  "critique": "...",
  "meta_summary": {"summary": "...", "diagnostic_impression": ["..."], "lessons": ["..."], "final_recommendation": "..."},
  "tokens": 7830,
  "latency_sec": 1.23
}
```

Final scene summaries appear as `snapshot_id: "scene-summary"` with `meta_summary` populated. Tokens and latency are sourced from the orchestrator metrics so downstream tooling can reason about compute cost and responsiveness.
